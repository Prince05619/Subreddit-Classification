{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TaD Coursework Skeleton 2020 - v1.0",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gv2i9fNDvrg",
        "colab_type": "text"
      },
      "source": [
        "## Part A: Subreddit Prediction ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG9BpbQt3-ko",
        "colab_type": "code",
        "outputId": "f9769fb8-938f-4144-da31-0672e3a8f01b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "subreddit_train = \"coursework_subreddit_train.json\"\n",
        "subreddit_validation = \"coursework_subreddit_validation.json\"\n",
        "subreddit_test = \"coursework_subreddit_test.json\"\n",
        "\n",
        "!gsutil cp gs://textasdata/coursework/coursework_subreddit_train2020.json $subreddit_train \n",
        "!gsutil cp gs://textasdata/coursework/coursework_subreddit_validation2020.json $subreddit_validation \n",
        "!gsutil cp gs://textasdata/coursework/coursework_subreddit_test.json  $subreddit_test"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://textasdata/coursework/coursework_subreddit_train2020.json...\n",
            "/ [1 files][  8.0 MiB/  8.0 MiB]                                                \n",
            "Operation completed over 1 objects/8.0 MiB.                                      \n",
            "Copying gs://textasdata/coursework/coursework_subreddit_validation2020.json...\n",
            "/ [1 files][  2.1 MiB/  2.1 MiB]                                                \n",
            "Operation completed over 1 objects/2.1 MiB.                                      \n",
            "Copying gs://textasdata/coursework/coursework_subreddit_test.json...\n",
            "/ [1 files][  2.7 MiB/  2.7 MiB]                                                \n",
            "Operation completed over 1 objects/2.7 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCEG8t6PC2f7",
        "colab_type": "code",
        "outputId": "4f10fbd5-b7ea-498c-a07e-1fd7df999390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_threads = pd.read_json(path_or_buf=subreddit_train, lines=True)\n",
        "print(list(train_threads.columns.values))\n",
        "print(train_threads.head())\n",
        "print(train_threads.size)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is_self_post', 'posts', 'subreddit', 'title', 'url']\n",
            "   is_self_post  ...                                                url\n",
            "0           1.0  ...  https://www.reddit.com/r/relationships/comment...\n",
            "1           1.0  ...  https://www.reddit.com/r/AskReddit/comments/22...\n",
            "2           1.0  ...  https://www.reddit.com/r/trees/comments/46d0iu...\n",
            "3           1.0  ...  https://www.reddit.com/r/AskReddit/comments/19...\n",
            "4           1.0  ...  https://www.reddit.com/r/explainlikeimfive/com...\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "5820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz5RFxFXDCKZ",
        "colab_type": "code",
        "outputId": "b3696a7e-b267-452f-a25e-88e974eb660a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "validation_threads = pd.read_json(path_or_buf=subreddit_validation, lines=True)\n",
        "print(list(validation_threads.columns.values))\n",
        "print(validation_threads.head())\n",
        "print(validation_threads.size)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is_self_post', 'posts', 'subreddit', 'title', 'url']\n",
            "   is_self_post  ...                                                url\n",
            "0           1.0  ...  https://www.reddit.com/r/hearthstone/comments/...\n",
            "1           1.0  ...  https://www.reddit.com/r/explainlikeimfive/com...\n",
            "2           1.0  ...  https://www.reddit.com/r/AskReddit/comments/tl...\n",
            "3           1.0  ...  https://www.reddit.com/r/AskReddit/comments/22...\n",
            "4           1.0  ...  https://www.reddit.com/r/reddit.com/comments/f...\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "1460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89UU3g27C8SZ",
        "colab_type": "code",
        "outputId": "13966692-c7fa-4e10-f6ce-11201c379fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "test_threads = pd.read_json(path_or_buf=subreddit_test, lines=True)\n",
        "print(test_threads.head())\n",
        "print(test_threads.size)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   is_self_post  ...                                                url\n",
            "0           1.0  ...  https://www.reddit.com/r/starcraft/comments/mq...\n",
            "1           1.0  ...  https://www.reddit.com/r/whowouldwin/comments/...\n",
            "2           1.0  ...  https://www.reddit.com/r/AskReddit/comments/27...\n",
            "3           1.0  ...  https://www.reddit.com/r/AskReddit/comments/x9...\n",
            "4           1.0  ...  https://www.reddit.com/r/tipofmytongue/comment...\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "1825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nl9qzazDQ_6",
        "colab_type": "code",
        "outputId": "0cb168f3-2e92-4c1d-af23-9493a8fb8c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "subreddit_counts = train_threads['subreddit'].value_counts()\n",
        "print(subreddit_counts.describe())\n",
        "top_subbreddits = subreddit_counts.nlargest(20)\n",
        "top_subbreddits_list = top_subbreddits.index.tolist()\n",
        "print(top_subbreddits)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count     20.000000\n",
            "mean      58.200000\n",
            "std       60.248258\n",
            "min       20.000000\n",
            "25%       28.750000\n",
            "50%       38.500000\n",
            "75%       49.500000\n",
            "max      276.000000\n",
            "Name: subreddit, dtype: float64\n",
            "askreddit               276\n",
            "leagueoflegends         157\n",
            "buildapc                103\n",
            "explainlikeimfive        60\n",
            "gaming                   51\n",
            "trees                    49\n",
            "techsupport              48\n",
            "pcmasterrace             47\n",
            "electronic_cigarette     46\n",
            "relationships            42\n",
            "tipofmytongue            35\n",
            "summonerschool           33\n",
            "jailbreak                31\n",
            "hearthstone              30\n",
            "whowouldwin              29\n",
            "atheism                  28\n",
            "reddit.com               27\n",
            "personalfinance          27\n",
            "movies                   25\n",
            "starcraft                20\n",
            "Name: subreddit, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02OXNfo9H8Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = train_threads['subreddit']\n",
        "validation_labels = validation_threads['subreddit']\n",
        "test_labels = test_threads['subreddit']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db53DOJCkqWH",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ban08pCbSAv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def eval_summary(predictions, labels, avg='macro'):\n",
        "    precision = precision_score(predictions, labels, average=avg)\n",
        "    recall = recall_score(predictions, labels, average=avg)\n",
        "    f1 = fbeta_score(predictions, labels, 1, average=avg)\n",
        "    accuracy = accuracy_score(predictions, labels)\n",
        "    print(\"Classifier  has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (accuracy,precision,recall,f1))\n",
        "    print(classification_report(predictions, labels, digits=3))\n",
        "    print('\\nConfusion matrix:\\n',confusion_matrix(labels, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2tgG8uTkxvC",
        "colab_type": "text"
      },
      "source": [
        "# Spacy 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuFxOt4YlBcY",
        "colab_type": "code",
        "outputId": "847441c4-32e1-45ef-f51c-974bdf5ca0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the medium english model. \n",
        "# We will use this model to get embedding features for tokens later.\n",
        "#!python -m spacy download en_core_web_md\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('parser')\n",
        "\n",
        "# Download a stopword list\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBqU-I30lCaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@Tokenize\n",
        "def spacy_tokenize(string):\n",
        "  tokens = list()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "#@Normalize\n",
        "def normalize(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    normalized = token.text.lower().strip()\n",
        "    if ((token.is_alpha or token.is_digit)):\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "  return normalized_tokens\n",
        "\n",
        "#@Tokenize and normalize\n",
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPRd-vbwlSU1",
        "colab_type": "text"
      },
      "source": [
        "#  Q1 Whole Thread Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq4qrKqFSA6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def field_thread(thread):\n",
        "    field_thread = [thread['title']]\n",
        "    for post in thread['posts']:\n",
        "        if \"body\" in post:\n",
        "            a = post['body'].strip()\n",
        "            if a != '':\n",
        "                field_thread.append(a)\n",
        "        if \"author\" in post:\n",
        "            b = post['author'].strip()\n",
        "            if b != '':\n",
        "                field_thread.append(b)       \n",
        "    return \" \".join(field_thread)\n",
        "  \n",
        "train_threads['full_thread'] = train_threads.apply(field_thread, axis=1)\n",
        "test_threads['full_thread'] = test_threads.apply(field_thread, axis=1)\n",
        "validation_threads['full_thread'] = validation_threads.apply(field_thread, axis=1)\n",
        "full_thread = pd.concat([train_threads['full_thread'], test_threads['full_thread'],validation_threads['full_thread']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewLuagqzlfQ7",
        "colab_type": "text"
      },
      "source": [
        "# Count Vectorizer & TFIDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKFgmhVnSA8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "count_vectorizer = CountVectorizer(tokenizer=tokenize_normalize)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_normalize)\n",
        "train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_threads.full_thread.values)\n",
        "train_cv_matrix = count_vectorizer.fit_transform(train_threads.full_thread.values)\n",
        "\n",
        "test_tfidf_matrix = tfidf_vectorizer.transform(test_threads.full_thread.values)\n",
        "test_cv_matrix = count_vectorizer.transform(test_threads.full_thread.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2EUt1-8l0uv",
        "colab_type": "text"
      },
      "source": [
        "# Dummy Classifier with strategy=\"most_frequent\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8N2md0ll8rm",
        "colab_type": "code",
        "outputId": "2b35fe97-d94e-4e30-a691-bd421e496954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "most_frequent_cv = DummyClassifier(strategy=\"most_frequent\")\n",
        "most_frequent_cv.fit(train_cv_matrix, train_labels)\n",
        "Most_Frequent= ['Dummy Most_Frequent', 'Count Vectorizer']\n",
        "prediction = most_frequent_cv.predict(test_cv_matrix)\n",
        "summary= eval_summary(prediction, test_labels, avg='macro')\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.230 P=0.050 R=0.012 F1=0.019\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      1.000     0.230     0.374       365\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.000     0.000     0.000         0\n",
            "electronic_cigarette      0.000     0.000     0.000         0\n",
            "   explainlikeimfive      0.000     0.000     0.000         0\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.000     0.000     0.000         0\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.000     0.000     0.000         0\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.000     0.000     0.000         0\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.000     0.000     0.000         0\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "            accuracy                          0.230       365\n",
            "           macro avg      0.050     0.012     0.019       365\n",
            "        weighted avg      1.000     0.230     0.374       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBubW_4ipIUr",
        "colab_type": "code",
        "outputId": "f7fc6126-feb0-4f4a-efd3-dd59ecf8fcae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "most_frequent_tf = DummyClassifier(strategy=\"most_frequent\")\n",
        "most_frequent_tf.fit(train_tfidf_matrix, train_labels)\n",
        "Most_Frequent1= ['Dummy Most_Frequent', 'Tf-idf Vectorizer']\n",
        "prediction = most_frequent_tf.predict(test_tfidf_matrix)\n",
        "summary1= eval_summary(prediction, test_labels, avg='macro')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.230 P=0.050 R=0.012 F1=0.019\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      1.000     0.230     0.374       365\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.000     0.000     0.000         0\n",
            "electronic_cigarette      0.000     0.000     0.000         0\n",
            "   explainlikeimfive      0.000     0.000     0.000         0\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.000     0.000     0.000         0\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.000     0.000     0.000         0\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.000     0.000     0.000         0\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.000     0.000     0.000         0\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "            accuracy                          0.230       365\n",
            "           macro avg      0.050     0.012     0.019       365\n",
            "        weighted avg      1.000     0.230     0.374       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kJjWkyqmHzY",
        "colab_type": "text"
      },
      "source": [
        "# Dummy Classifier with strategy=\"stratified\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JidiDVzOmNon",
        "colab_type": "code",
        "outputId": "845d5aec-6ad5-4f71-de90-82ce1bfb6155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "stratified_cv = DummyClassifier(strategy=\"stratified\")\n",
        "stratified_cv.fit(train_cv_matrix, train_labels)\n",
        "DS = ['Dummy Stratified', 'Count Vectorizer']\n",
        "prediction = stratified_cv.predict(test_cv_matrix)\n",
        "summary2 = eval_summary(prediction, test_labels, avg='macro')\n",
        "\n",
        "stratified_tf = DummyClassifier(strategy=\"stratified\")\n",
        "stratified_tf.fit(train_tfidf_matrix, train_labels)\n",
        "DS1= ['Dummy Stratified', 'Tf-idf Vectorizer']\n",
        "prediction = stratified_tf.predict(test_tfidf_matrix)\n",
        "summary3 = eval_summary(prediction, test_labels, avg='macro')\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.082 P=0.033 R=0.033 F1=0.033\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.226     0.207     0.216        92\n",
            "             atheism      0.000     0.000     0.000        11\n",
            "            buildapc      0.054     0.080     0.065        25\n",
            "electronic_cigarette      0.000     0.000     0.000        12\n",
            "   explainlikeimfive      0.000     0.000     0.000        20\n",
            "              gaming      0.000     0.000     0.000        23\n",
            "         hearthstone      0.000     0.000     0.000         8\n",
            "           jailbreak      0.000     0.000     0.000        13\n",
            "     leagueoflegends      0.146     0.135     0.140        52\n",
            "              movies      0.000     0.000     0.000        10\n",
            "        pcmasterrace      0.000     0.000     0.000         9\n",
            "     personalfinance      0.000     0.000     0.000        10\n",
            "          reddit.com      0.167     0.167     0.167         6\n",
            "       relationships      0.000     0.000     0.000         7\n",
            "           starcraft      0.000     0.000     0.000         6\n",
            "      summonerschool      0.000     0.000     0.000         5\n",
            "         techsupport      0.077     0.062     0.069        16\n",
            "       tipofmytongue      0.000     0.000     0.000        11\n",
            "               trees      0.000     0.000     0.000        17\n",
            "         whowouldwin      0.000     0.000     0.000        12\n",
            "\n",
            "            accuracy                          0.082       365\n",
            "           macro avg      0.033     0.033     0.033       365\n",
            "        weighted avg      0.088     0.082     0.085       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[19  4  2  3  3  3  1  2 17  3  3  4  1  3  2  1  3  0  5  5]\n",
            " [ 5  0  2  0  0  0  0  1  1  0  0  1  0  0  1  1  0  0  0  0]\n",
            " [11  3  2  1  3  3  1  0  5  1  2  0  2  0  0  1  0  0  1  1]\n",
            " [ 5  0  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  1  0  1]\n",
            " [ 3  1  2  1  0  1  0  1  1  1  0  0  0  0  1  0  1  0  1  0]\n",
            " [ 4  0  1  1  1  0  0  0  0  0  2  1  1  1  1  0  0  1  3  0]\n",
            " [ 2  0  0  2  1  2  0  2  0  0  0  0  0  0  0  1  1  2  1  1]\n",
            " [ 2  0  3  0  2  1  0  0  1  0  0  0  0  1  0  1  0  0  0  0]\n",
            " [16  0  1  2  2  3  1  1  7  1  1  2  0  0  1  0  5  4  1  0]\n",
            " [ 1  0  1  0  1  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  1  1  2  3  1  2  3  3  0  0  0  0  1  0  0  1  1  0  0]\n",
            " [ 2  0  1  0  1  1  1  1  1  0  0  0  0  0  0  0  1  0  1  0]\n",
            " [ 0  0  2  0  1  0  1  0  1  0  0  0  1  0  0  0  0  0  0  0]\n",
            " [ 1  0  2  0  0  0  0  0  0  0  0  0  1  0  0  0  1  0  1  0]\n",
            " [ 4  1  0  0  0  0  0  0  3  1  0  0  0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  1  1  1  0  1  0  1  0  0  0  0  0  0  0  0  1]\n",
            " [ 2  0  2  0  0  1  0  0  2  1  0  2  0  0  0  0  1  2  0  0]\n",
            " [ 3  0  0  0  1  0  0  1  2  1  0  0  0  0  0  0  0  0  1  2]\n",
            " [ 6  0  3  0  0  3  0  0  5  1  0  0  0  0  0  0  1  0  0  1]\n",
            " [ 2  1  0  0  0  1  0  1  1  0  0  0  0  0  0  0  0  0  2  0]]\n",
            "Classifier  has Acc=0.099 P=0.046 R=0.045 F1=0.044\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.226     0.176     0.198       108\n",
            "             atheism      0.000     0.000     0.000         8\n",
            "            buildapc      0.108     0.160     0.129        25\n",
            "electronic_cigarette      0.000     0.000     0.000         8\n",
            "   explainlikeimfive      0.143     0.100     0.118        20\n",
            "              gaming      0.000     0.000     0.000        10\n",
            "         hearthstone      0.000     0.000     0.000         6\n",
            "           jailbreak      0.000     0.000     0.000        11\n",
            "     leagueoflegends      0.125     0.113     0.119        53\n",
            "              movies      0.000     0.000     0.000         6\n",
            "        pcmasterrace      0.043     0.062     0.051        16\n",
            "     personalfinance      0.000     0.000     0.000         6\n",
            "          reddit.com      0.000     0.000     0.000         2\n",
            "       relationships      0.000     0.000     0.000        15\n",
            "           starcraft      0.000     0.000     0.000         8\n",
            "      summonerschool      0.000     0.000     0.000        12\n",
            "         techsupport      0.077     0.083     0.080        12\n",
            "       tipofmytongue      0.091     0.083     0.087        12\n",
            "               trees      0.100     0.118     0.108        17\n",
            "         whowouldwin      0.000     0.000     0.000        10\n",
            "\n",
            "            accuracy                          0.099       365\n",
            "           macro avg      0.046     0.045     0.044       365\n",
            "        weighted avg      0.112     0.099     0.104       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[19  1  7  3  4  1  1  2 18  2  6  1  0  4  4  2  2  3  1  3]\n",
            " [ 3  0  1  0  2  0  0  1  2  0  1  0  0  0  0  0  0  0  1  1]\n",
            " [ 9  0  4  1  1  1  2  2  5  1  1  1  1  1  1  1  0  1  3  1]\n",
            " [ 1  0  0  0  1  0  0  0  2  0  0  1  0  0  0  2  0  1  1  0]\n",
            " [ 5  1  1  0  2  0  0  0  1  0  0  0  0  2  0  0  2  0  0  0]\n",
            " [ 6  0  1  0  1  0  0  1  1  0  1  0  1  0  0  1  1  0  2  1]\n",
            " [ 5  2  2  0  0  1  0  0  4  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0]\n",
            " [15  0  1  0  4  2  2  1  6  0  1  0  0  4  1  2  2  3  3  1]\n",
            " [ 0  0  0  0  3  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0]\n",
            " [ 6  0  1  0  2  3  0  1  5  0  1  1  0  1  0  0  0  0  1  1]\n",
            " [ 6  0  1  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  1  0]\n",
            " [ 2  0  0  0  0  0  0  1  1  0  1  0  0  0  0  1  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  2  1  0  0]\n",
            " [ 2  0  1  1  0  1  0  0  1  1  0  0  0  1  0  1  1  0  0  0]\n",
            " [ 0  1  0  2  0  0  0  0  1  0  0  1  0  0  0  0  0  0  1  0]\n",
            " [ 5  0  1  1  0  0  0  0  1  0  1  0  0  0  1  1  1  0  0  1]\n",
            " [ 3  2  1  0  0  0  0  0  1  0  1  0  0  1  0  0  0  1  1  0]\n",
            " [ 7  1  3  0  0  0  1  0  2  0  0  0  0  0  0  0  1  2  2  1]\n",
            " [ 4  0  0  0  0  0  0  1  0  1  0  1  0  0  1  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmycbcmJmWO8",
        "colab_type": "text"
      },
      "source": [
        "# LogisticRegression with One-hot vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r9hTrN4mbwE",
        "colab_type": "code",
        "outputId": "a4932c66-5829-41d0-abd5-d8b2b82ca80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_cv = LogisticRegression()\n",
        "lr_cv.fit(train_cv_matrix, train_labels)\n",
        "LR= ['LogisticRegression', 'Count Vectorizer']\n",
        "prediction = lr_cv.predict(test_cv_matrix)\n",
        "print(LR)\n",
        "summary4 = eval_summary(prediction, test_labels, avg='macro')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['LogisticRegression', 'Count Vectorizer']\n",
            "Classifier  has Acc=0.597 P=0.479 R=0.585 F1=0.504\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.833     0.560     0.670       125\n",
            "             atheism      0.417     0.556     0.476         9\n",
            "            buildapc      0.730     0.730     0.730        37\n",
            "electronic_cigarette      0.667     0.667     0.667         9\n",
            "   explainlikeimfive      0.500     0.636     0.560        11\n",
            "              gaming      0.471     0.381     0.421        21\n",
            "         hearthstone      0.267     0.800     0.400         5\n",
            "           jailbreak      0.545     1.000     0.706         6\n",
            "     leagueoflegends      0.833     0.615     0.708        65\n",
            "              movies      0.200     0.500     0.286         2\n",
            "        pcmasterrace      0.174     0.444     0.250         9\n",
            "     personalfinance      0.800     1.000     0.889         8\n",
            "          reddit.com      0.000     0.000     0.000         2\n",
            "       relationships      0.667     0.571     0.615         7\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.333     0.667     0.444         3\n",
            "         techsupport      0.615     0.471     0.533        17\n",
            "       tipofmytongue      0.545     0.462     0.500        13\n",
            "               trees      0.350     0.636     0.452        11\n",
            "         whowouldwin      0.625     1.000     0.769         5\n",
            "\n",
            "            accuracy                          0.597       365\n",
            "           macro avg      0.479     0.585     0.504       365\n",
            "        weighted avg      0.695     0.597     0.625       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[70  2  0  1  2  3  1  0  2  0  1  0  0  1  0  0  0  0  1  0]\n",
            " [ 5  5  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  0 27  0  0  0  0  0  1  0  2  0  0  0  0  0  3  0  0  0]\n",
            " [ 1  0  0  6  0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0]\n",
            " [ 7  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  1  0  0  8  0  0  1  0  1  0  0  0  0  0  0  3  0  0]\n",
            " [ 3  0  0  0  0  1  4  0  5  0  0  0  0  1  0  0  0  0  1  0]\n",
            " [ 3  0  0  0  0  0  0  6  2  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  1  0  0  1  0  0 40  0  0  0  0  1  0  1  0  0  2  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0]\n",
            " [ 2  0  6  0  0  4  0  0  2  0  4  0  0  0  0  0  5  0  0  0]\n",
            " [ 1  0  0  0  1  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]\n",
            " [ 4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]\n",
            " [ 2  0  0  1  0  3  0  0  2  0  0  0  0  0  0  0  0  2  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  2  0  0  0  0]\n",
            " [ 2  0  2  1  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0]\n",
            " [ 4  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  6  0  0]\n",
            " [ 5  0  0  0  0  1  0  0  6  0  0  0  0  0  0  0  0  1  7  0]\n",
            " [ 2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Xk0azpmmSE",
        "colab_type": "text"
      },
      "source": [
        "# LogisticRegression with TF-IDF vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rejKLaGVmq9X",
        "colab_type": "code",
        "outputId": "ae3ff76b-f1f2-4fc7-bf69-7898e4bbc9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "lr_tfidf = LogisticRegression()\n",
        "lr_tfidf.fit(train_tfidf_matrix, train_labels)\n",
        "LR1 = ['LogisticRegression', 'Tf-idf Vectorizer']\n",
        "prediction = lr_tfidf.predict(test_tfidf_matrix)\n",
        "summary5= eval_summary(prediction, test_labels, avg='macro')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.477 P=0.256 R=0.396 F1=0.268\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.952     0.340     0.502       235\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.730     0.794     0.761        34\n",
            "electronic_cigarette      0.556     1.000     0.714         5\n",
            "   explainlikeimfive      0.071     0.500     0.125         2\n",
            "              gaming      0.000     0.000     0.000         2\n",
            "         hearthstone      0.067     1.000     0.125         1\n",
            "           jailbreak      0.273     1.000     0.429         3\n",
            "     leagueoflegends      0.875     0.656     0.750        64\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.400     1.000     0.571         4\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.667     1.000     0.800         4\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.538     0.636     0.583        11\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "            accuracy                          0.477       365\n",
            "           macro avg      0.256     0.396     0.268       365\n",
            "        weighted avg      0.873     0.477     0.572       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[80  0  0  0  1  2  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 7  0 27  0  0  0  0  0  1  0  0  0  0  0  0  0  2  0  0  0]\n",
            " [ 4  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [13  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [13  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  1  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0 42  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  6  0  0  0  0  0  4  0  0  0  0  0  0  0  2  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 7  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A24AcTsymx9s",
        "colab_type": "text"
      },
      "source": [
        "# SVC Classifier with One-hot vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjy9ZYYvm3DY",
        "colab_type": "code",
        "outputId": "38a026b8-1f17-46a5-9792-87bb8242aca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "svc_cv = SVC()\n",
        "svc_cv.fit(train_cv_matrix, train_labels)\n",
        "SVC1= ['SVM Classifier', 'Count Vectorizer']\n",
        "prediction = svc_cv.predict(test_cv_matrix)\n",
        "summary6 = eval_summary(prediction, test_labels, avg='macro')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.329 P=0.159 R=0.200 F1=0.140\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.964     0.280     0.434       289\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.486     0.857     0.621        21\n",
            "electronic_cigarette      0.000     0.000     0.000         1\n",
            "   explainlikeimfive      0.357     0.385     0.370        13\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.125     0.400     0.190        15\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.100     1.000     0.182         1\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.667     0.308     0.421        13\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.167     0.200     0.182         5\n",
            "         techsupport      0.308     0.571     0.400         7\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "            accuracy                          0.329       365\n",
            "           macro avg      0.159     0.200     0.140       365\n",
            "        weighted avg      0.842     0.329     0.426       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[81  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0]\n",
            " [14  0 18  0  1  0  0  0  0  0  0  0  0  2  0  0  2  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  2  0  0  0  0  1  0  1  0  0  0  0]\n",
            " [10  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [42  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  3  0  0  0  0  0  2  0  0  0  0  0  0  0  1  0  0  0]\n",
            " [ 4  0  0  0  0  0  0  0  0  0  0  1  0  2  0  3  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]\n",
            " [ 8  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0  2  0  0  0  0  1  0  1  0  0  0  0]\n",
            " [ 8  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpY6Vc5Rm-eM",
        "colab_type": "text"
      },
      "source": [
        "# An interesting classifier model : Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afXGj5JGSA99",
        "colab_type": "code",
        "outputId": "1b9a8570-8944-47a1-d10c-85e9ab75b8c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt_tfidf = DecisionTreeClassifier()\n",
        "dt_tfidf.fit(train_tfidf_matrix, train_labels)\n",
        "Tree= ['decision', 'Tf-idf Vectorizer']\n",
        "prediction = dt_tfidf.predict(test_tfidf_matrix)\n",
        "summary7= eval_summary(prediction, test_labels, avg='macro')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.430 P=0.347 R=0.364 F1=0.347\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.607     0.531     0.567        96\n",
            "             atheism      0.083     0.077     0.080        13\n",
            "            buildapc      0.568     0.700     0.627        30\n",
            "electronic_cigarette      0.333     0.429     0.375         7\n",
            "   explainlikeimfive      0.214     0.214     0.214        14\n",
            "              gaming      0.353     0.286     0.316        21\n",
            "         hearthstone      0.267     0.286     0.276        14\n",
            "           jailbreak      0.455     0.556     0.500         9\n",
            "     leagueoflegends      0.479     0.535     0.505        43\n",
            "              movies      0.000     0.000     0.000         5\n",
            "        pcmasterrace      0.043     0.056     0.049        18\n",
            "     personalfinance      0.400     0.800     0.533         5\n",
            "          reddit.com      0.000     0.000     0.000        13\n",
            "       relationships      0.667     0.500     0.571         8\n",
            "           starcraft      0.100     0.167     0.125         6\n",
            "      summonerschool      0.333     0.250     0.286         8\n",
            "         techsupport      0.769     0.714     0.741        14\n",
            "       tipofmytongue      0.636     0.467     0.538        15\n",
            "               trees      0.500     0.455     0.476        22\n",
            "         whowouldwin      0.125     0.250     0.167         4\n",
            "\n",
            "            accuracy                          0.430       365\n",
            "           macro avg      0.347     0.364     0.347       365\n",
            "        weighted avg      0.440     0.430     0.431       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[51  4  0  0  3  4  3  1  5  2  3  0  3  2  0  0  0  1  2  0]\n",
            " [ 6  1  0  1  0  0  1  0  0  0  0  1  0  1  0  0  0  1  0  0]\n",
            " [ 4  0 21  0  0  0  2  1  0  1  4  0  1  0  1  0  0  0  1  1]\n",
            " [ 1  3  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]\n",
            " [ 4  1  0  0  3  0  0  2  0  1  0  0  0  0  2  0  0  0  0  1]\n",
            " [ 3  0  0  0  0  6  0  0  3  0  4  0  0  0  0  0  0  1  0  0]\n",
            " [ 1  1  2  0  0  3  4  0  2  0  0  0  0  0  1  0  0  0  1  0]\n",
            " [ 2  0  0  0  0  0  0  5  1  0  0  0  1  1  1  0  0  0  0  0]\n",
            " [ 6  0  1  0  1  3  2  0 23  0  3  0  2  0  0  3  0  1  3  0]\n",
            " [ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
            " [ 0  1  5  1  0  4  1  0  2  0  1  0  1  0  0  1  4  2  0  0]\n",
            " [ 1  0  0  1  2  0  0  0  0  0  1  4  0  0  0  1  0  0  0  0]\n",
            " [ 3  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]\n",
            " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  4  0  1  0  0  0  0]\n",
            " [ 1  1  0  0  0  0  0  0  3  0  1  0  3  0  1  0  0  0  0  0]\n",
            " [ 2  0  0  0  1  0  0  0  1  0  0  0  0  0  0  2  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  1  0  0  0  0  0  1  0  0  0 10  0  0  0]\n",
            " [ 1  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  7  1  0]\n",
            " [ 5  0  0  1  1  1  0  0  1  0  0  0  0  0  0  0  0  0 10  1]\n",
            " [ 1  0  0  0  1  0  0  0  2  1  1  0  0  0  0  0  0  1  0  1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi6PU8iKqQwa",
        "colab_type": "text"
      },
      "source": [
        "# Q2 Parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHMeZs3ezkKh",
        "colab_type": "code",
        "outputId": "9411cd65-3019-4a72-e62c-5672e74eb6d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "prediction_pipeline = Pipeline([\n",
        "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)),\n",
        "              ('logreg', LogisticRegression())\n",
        "              ])\n",
        "prediction_pipeline.fit(train_threads.full_thread.values, train_labels)\n",
        "    "
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u...',\n",
              "                                 tokenizer=<function tokenize_normalize at 0x7f14d34d5268>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('logreg',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIE8EJIJz7U-",
        "colab_type": "code",
        "outputId": "dce32e6c-d95e-4ef9-b626-a0ea3e31d61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    'tf__sublinear_tf': [True],\n",
        "    'tf__ngram_range': [(1,1)],\n",
        "    'tf__max_features': [5000],\n",
        "    'logreg__C': [100]\n",
        "} \n",
        "grid_search = GridSearchCV(prediction_pipeline, param_grid=params, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "grid_search.fit(train_threads.full_thread.values, train_labels)\n",
        "best_estimator = grid_search.best_estimator_\n",
        "best_estimator_test_predict = best_estimator.predict(test_threads.full_thread.values)\n",
        "best_estimator_validation_predict = best_estimator.predict(validation_threads.full_thread.values)\n",
        "summary8=eval_summary(best_estimator_test_predict, test_labels)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   11.6s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.701 P=0.587 R=0.776 F1=0.635\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.917     0.579     0.710       133\n",
            "             atheism      0.417     0.833     0.556         6\n",
            "            buildapc      0.838     0.756     0.795        41\n",
            "electronic_cigarette      0.778     1.000     0.875         7\n",
            "   explainlikeimfive      0.571     0.727     0.640        11\n",
            "              gaming      0.647     0.458     0.537        24\n",
            "         hearthstone      0.600     1.000     0.750         9\n",
            "           jailbreak      0.727     1.000     0.842         8\n",
            "     leagueoflegends      0.875     0.750     0.808        56\n",
            "              movies      0.200     0.500     0.286         2\n",
            "        pcmasterrace      0.261     0.857     0.400         7\n",
            "     personalfinance      0.800     1.000     0.889         8\n",
            "          reddit.com      0.000     0.000     0.000         1\n",
            "       relationships      1.000     0.750     0.857         8\n",
            "           starcraft      0.300     1.000     0.462         3\n",
            "      summonerschool      0.333     0.667     0.444         3\n",
            "         techsupport      0.846     0.647     0.733        17\n",
            "       tipofmytongue      0.636     1.000     0.778         7\n",
            "               trees      0.500     1.000     0.667        10\n",
            "         whowouldwin      0.500     1.000     0.667         4\n",
            "\n",
            "            accuracy                          0.701       365\n",
            "           macro avg      0.587     0.776     0.635       365\n",
            "        weighted avg      0.796     0.701     0.719       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[77  0  0  0  2  4  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 7  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0 31  0  0  0  0  0  1  0  1  0  0  0  0  0  2  0  0  0]\n",
            " [ 2  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  8  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0 11  0  0  3  0  0  0  0  0  0  0  1  0  0  0]\n",
            " [ 3  0  0  0  0  1  9  0  1  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  8  2  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  1  0  0  2  0  0 42  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  7  0  0  4  0  0  1  0  6  0  0  0  0  0  3  0  0  0]\n",
            " [ 1  0  0  0  1  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  1  0  0  2  0  0  0  1  0  3  1  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  2  0  0  0  0]\n",
            " [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  7  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0]\n",
            " [ 3  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5yXEO2-X49",
        "colab_type": "text"
      },
      "source": [
        "# For error findings compare with validation.predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgECUYyb2aMc",
        "colab_type": "code",
        "outputId": "6af1fd0c-9093-4e2e-985c-f7b0ddafd178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "best_estimator_validation_predict = best_estimator.predict(validation_threads.full_thread.values)\n",
        "summary9=eval_summary(best_estimator_validation_predict,validation_labels)\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.688 P=0.605 R=0.786 F1=0.638\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.966     0.514     0.671       109\n",
            "             atheism      0.444     1.000     0.615         4\n",
            "            buildapc      0.821     0.767     0.793        30\n",
            "electronic_cigarette      0.846     1.000     0.917        11\n",
            "   explainlikeimfive      0.409     0.600     0.486        15\n",
            "              gaming      0.636     0.700     0.667        10\n",
            "         hearthstone      0.375     1.000     0.545         3\n",
            "           jailbreak      0.714     0.833     0.769         6\n",
            "     leagueoflegends      0.897     0.761     0.824        46\n",
            "              movies      0.125     1.000     0.222         1\n",
            "        pcmasterrace      0.333     0.714     0.455         7\n",
            "     personalfinance      1.000     0.714     0.833         7\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.833     1.000     0.909         5\n",
            "           starcraft      0.250     1.000     0.400         2\n",
            "      summonerschool      0.750     0.600     0.667         5\n",
            "         techsupport      0.667     0.769     0.714        13\n",
            "       tipofmytongue      0.875     1.000     0.933         7\n",
            "               trees      0.412     1.000     0.583         7\n",
            "         whowouldwin      0.750     0.750     0.750         4\n",
            "\n",
            "            accuracy                          0.688       292\n",
            "           macro avg      0.605     0.786     0.638       292\n",
            "        weighted avg      0.818     0.688     0.714       292\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[56  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
            " [ 5  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0 23  0  1  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]\n",
            " [ 1  0  0 11  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  9  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  0  7  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  3  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  5  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0 35  0  0  0  0  0  0  2  0  0  0  0]\n",
            " [ 6  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0  6  0  0  3  0  0  0  0  5  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0]\n",
            " [ 7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  1  0  0  0  3  0  0  0  0  0  2  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  3  0  0  0  0]\n",
            " [ 1  0  1  0  0  0  0  1  0  0  2  0  0  0  0  0 10  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0]\n",
            " [ 7  0  0  0  1  0  0  0  2  0  0  0  0  0  0  0  0  0  7  0]\n",
            " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBYwHiIgxIm5",
        "colab_type": "text"
      },
      "source": [
        "# Q3 Adding Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_OIA2jS4AVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_thread = list()\n",
        "temp=list()\n",
        "import json \n",
        "with open(subreddit_train) as jsonfile:\n",
        "  for i, line in enumerate(jsonfile):\n",
        "    thread = json.loads(line)\n",
        "    id = list()\n",
        "    author = list()\n",
        "    body = list()\n",
        "    in_reply_to=list()\n",
        "    for post in thread['posts']:\n",
        "      id.append(post.get('id', 0))\n",
        "      author.append(post.get('author', \"\"))\n",
        "      body.append(post.get('body', \"\"))\n",
        "      in_reply_to.append(post.get('in_reply_to',\"\"))\n",
        "    temp.append((thread['title'],' '.join(id), ' '.join(author), ' '.join(body),' '.join(in_reply_to)))\n",
        "    train_thread.extend(temp)\n",
        "    temp.clear()\n",
        "labels = ['title',  'id', 'author', 'body','in_reply_to']\n",
        "trainDf= pd.DataFrame(train_thread, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8LMwhZ-2lt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_thread = list()\n",
        "temp=list()\n",
        "with open(subreddit_test) as jsonfile:\n",
        "  for i, line in enumerate(jsonfile):\n",
        "    thread = json.loads(line)\n",
        "    id = list()\n",
        "    author = list()\n",
        "    body = list()\n",
        "    in_reply_to=list()\n",
        "    for post in thread['posts']:\n",
        "      id.append(post.get('id', 0))\n",
        "      author.append(post.get('author', \"\"))\n",
        "      body.append(post.get('body', \"\"))\n",
        "      in_reply_to.append(post.get('in_reply_to',\"\"))\n",
        "    temp.append((thread['title'],' '.join(id), ' '.join(author), ' '.join(body),' '.join(in_reply_to)))\n",
        "    test_thread.extend(temp)\n",
        "    temp.clear()\n",
        "labels = ['title', 'id', 'author', 'body','in_reply_to']\n",
        "testDf= pd.DataFrame(test_thread, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5WxXAIB2tHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SplR7n3kSBBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "prediction_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('Tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=True,ngram_range=(1,1),max_features=5000)), \n",
        "              ])),\n",
        "            ('id', Pipeline([\n",
        "              ('selector', ItemSelector(key='id')),\n",
        "              ('Tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=True,ngram_range=(1,1),max_features=5000)), \n",
        "              ])),\n",
        "             ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('vectorizer', TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf= True, max_features=5000, ngram_range=(1,1)))\n",
        "              ])),\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('vectorizer', TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf= True, max_features=5000, ngram_range=(1,1)))\n",
        "              ])),\n",
        "            ('in_reply_to', Pipeline([\n",
        "              ('selector', ItemSelector(key='in_reply_to')),\n",
        "              ('Tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=True,ngram_range=(1,1),max_features=5000)), \n",
        "              ])), \n",
        "        ])\n",
        "        )\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3bCioaPwTgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features= prediction_pipeline.fit_transform(trainDf)\n",
        "test_features = prediction_pipeline.transform(testDf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44UzAJlQwTnq",
        "colab_type": "code",
        "outputId": "3e6d8e75-1822-40f8-e6a0-35c3bc1ab800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr3 = LogisticRegression(C=100)\n",
        "combined_model = lr3.fit(train_features,train_labels)\n",
        "summary10=eval_summary(lr3.predict(test_features), test_labels) \n",
        " "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.660 P=0.520 R=0.663 F1=0.557\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.964     0.551     0.701       147\n",
            "             atheism      0.500     1.000     0.667         6\n",
            "            buildapc      0.811     0.811     0.811        37\n",
            "electronic_cigarette      0.889     0.889     0.889         9\n",
            "   explainlikeimfive      0.429     0.857     0.571         7\n",
            "              gaming      0.294     0.455     0.357        11\n",
            "         hearthstone      0.533     1.000     0.696         8\n",
            "           jailbreak      0.636     1.000     0.778         7\n",
            "     leagueoflegends      0.896     0.632     0.741        68\n",
            "              movies      0.200     0.500     0.286         2\n",
            "        pcmasterrace      0.174     0.444     0.250         9\n",
            "     personalfinance      0.900     1.000     0.947         9\n",
            "          reddit.com      0.000     0.000     0.000         2\n",
            "       relationships      1.000     1.000     1.000         6\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.692     0.600     0.643        15\n",
            "       tipofmytongue      0.455     1.000     0.625         5\n",
            "               trees      0.400     0.889     0.552         9\n",
            "         whowouldwin      0.625     0.625     0.625         8\n",
            "\n",
            "            accuracy                          0.660       365\n",
            "           macro avg      0.520     0.663     0.557       365\n",
            "        weighted avg      0.811     0.660     0.697       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[81  0  0  0  0  2  0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
            " [ 5  6  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  0 30  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  1]\n",
            " [ 1  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  5  0  0  7  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  1  0  0  2  8  0  1  0  0  0  0  0  0  0  0  0  1  0]\n",
            " [ 2  0  0  0  0  0  0  7  0  0  1  0  0  0  0  0  1  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0 43  0  1  0  0  0  0  0  1  0  0  0]\n",
            " [ 4  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 7  0  5  0  0  1  0  0  3  0  4  0  0  0  0  0  2  0  0  1]\n",
            " [ 1  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0]\n",
            " [ 3  0  0  1  0  1  0  0  4  0  0  0  0  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  0  0  1  0  2  0  0  0  0  0  9  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  5  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  8  0]\n",
            " [ 2  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  5]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-G938e8D5qn",
        "colab_type": "text"
      },
      "source": [
        "## Part B: Discourse prediction ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KM6aJtSETPY",
        "colab_type": "code",
        "outputId": "ce4d81b9-c865-4181-d6e7-c1ea4cecaa32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "discourse_train = \"coursework_discourse_train.json\"\n",
        "discourse_test = \"coursework_discourse_test.json\"\n",
        "  \n",
        "!gsutil cp gs://textasdata/coursework/coursework_discourse_train.json $discourse_train  \n",
        "!gsutil cp gs://textasdata/coursework/coursework_discourse_test.json  $discourse_test"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://textasdata/coursework/coursework_discourse_train.json...\n",
            "| [1 files][ 60.2 MiB/ 60.2 MiB]                                                \n",
            "Operation completed over 1 objects/60.2 MiB.                                     \n",
            "Copying gs://textasdata/coursework/coursework_discourse_test.json...\n",
            "\\ [1 files][ 15.1 MiB/ 15.1 MiB]                                                \n",
            "Operation completed over 1 objects/15.1 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwOaf_6aD-Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The reddit thread structure is nested with posts in a new content.\n",
        "# This block reads the file as json and creates a new data frame.\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def load_posts(file):\n",
        "  # A temporary variable to store the list of post content.\n",
        "  posts_tmp = list()\n",
        "\n",
        "  with open(file) as jsonfile:\n",
        "    for i, line in enumerate(jsonfile):\n",
        "     # if (i > 2): break\n",
        "      thread = json.loads(line)\n",
        "      for post in thread['posts']:\n",
        "        # NOTE: This should be changed to use additional features from the post or thread.\n",
        "        # DO NOT change the labels for the test set.\n",
        "        posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
        "                        post['id'], post.get('author', \"\"), post.get('body', \"\"), post.get(\"majority_link\", \"\"), \n",
        "                        post.get('post_depth', 0), post.get('majority_type', \"\"), # discourse type label \n",
        "                        post.get('in_reply_to', \"\") ))\n",
        "\n",
        "# Create the posts data frame.  \n",
        "  labels = ['subreddit', 'title', 'url', 'id', 'author', 'body', 'majority_link', \n",
        "          'post_depth', 'discourse_type', 'in_reply_to']\n",
        "  return pd.DataFrame(posts_tmp, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDzHTDcmEQ11",
        "colab_type": "code",
        "outputId": "5348cdee-77df-44f1-9aed-530d6657542f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "original_train_posts = load_posts(discourse_train)\n",
        "# Filter out empty labels\n",
        "original_train_posts = original_train_posts[original_train_posts['discourse_type'] != \"\"]\n",
        "print(original_train_posts.head())\n",
        "print(\"Num posts: \", original_train_posts.size)\n",
        "print(len(original_train_posts))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    subreddit                           title  ... discourse_type in_reply_to\n",
            "0  worldofpvp  Help me decide my new PvP main  ...       question            \n",
            "1  worldofpvp  Help me decide my new PvP main  ...         answer   t3_2v0anq\n",
            "2  worldofpvp  Help me decide my new PvP main  ...         answer   t3_2v0anq\n",
            "3  worldofpvp  Help me decide my new PvP main  ...         answer   t3_2v0anq\n",
            "4  worldofpvp  Help me decide my new PvP main  ...         answer   t3_2v0anq\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "Num posts:  792670\n",
            "79267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmIeAX9lFIkp",
        "colab_type": "text"
      },
      "source": [
        "#### Development / Validation data\n",
        "\n",
        "For part B it is up to you to split the \"original\" training data into a new train/validation (development) dataset appropriately. See Lab 4 for an example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIauHZBbFuqT",
        "colab_type": "code",
        "outputId": "90b585ea-520d-4300-d19f-9c1148b30c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "\n",
        "train_split = int(len(original_train_posts))\n",
        "tmp_train = original_train_posts.iloc[:train_split,:]\n",
        "#Split the train data into a train/validation split that's 80% train, 20% developemnt \n",
        "validation_split = int(train_split * 0.8)\n",
        "train_posts = tmp_train.iloc[:validation_split,:]\n",
        "validation_posts= tmp_train.iloc[validation_split:,:]\n",
        "print(len(validation_posts))\n",
        "print((train_posts))\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15854\n",
            "        subreddit                           title  ... discourse_type in_reply_to\n",
            "0      worldofpvp  Help me decide my new PvP main  ...       question            \n",
            "1      worldofpvp  Help me decide my new PvP main  ...         answer   t3_2v0anq\n",
            "2      worldofpvp  Help me decide my new PvP main  ...         answer   t3_2v0anq\n",
            "3      worldofpvp  Help me decide my new PvP main  ...         answer   t3_2v0anq\n",
            "4      worldofpvp  Help me decide my new PvP main  ...         answer   t3_2v0anq\n",
            "...           ...                             ...  ...            ...         ...\n",
            "70778   starcraft             Mechanical Keyboard  ...         answer    t3_k0qoo\n",
            "70779   starcraft             Mechanical Keyboard  ...       question  t1_c2gnfdx\n",
            "70780   starcraft             Mechanical Keyboard  ...         answer  t1_c2goe63\n",
            "70781   starcraft             Mechanical Keyboard  ...         answer    t3_k0qoo\n",
            "70782   starcraft             Mechanical Keyboard  ...         answer    t3_k0qoo\n",
            "\n",
            "[63413 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSA0KrvkFmLg",
        "colab_type": "text"
      },
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vvo7hfCEmvj",
        "colab_type": "code",
        "outputId": "b19b53bd-5cd1-4295-d155-7394c4ad9b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "test_posts = load_posts(discourse_test)\n",
        "# Filter out empty labels\n",
        "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
        "print(test_posts)\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         subreddit  ... in_reply_to\n",
            "0      photography  ...            \n",
            "1      photography  ...   t3_1ds5ds\n",
            "2      photography  ...  t1_c9tbz9b\n",
            "3      photography  ...  t1_c9tcqh8\n",
            "4      photography  ...  t1_c9tbz9b\n",
            "...            ...  ...         ...\n",
            "22092  picrequests  ...   t3_163amt\n",
            "22093  picrequests  ...  t1_c7sk2dc\n",
            "22094  picrequests  ...   t3_163amt\n",
            "22095  picrequests  ...  t1_c7supz7\n",
            "22096  picrequests  ...   t3_163amt\n",
            "\n",
            "[19812 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g08g8W1tG_es",
        "colab_type": "text"
      },
      "source": [
        "### Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqGLzyTOGadY",
        "colab_type": "text"
      },
      "source": [
        "The label for the post we will be predicting is in the discourse_type column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jat55HhNHGBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = train_posts['discourse_type']\n",
        "validation_labels = validation_posts['discourse_type']\n",
        "test_labels = test_posts['discourse_type']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFl6sM58HNFp",
        "colab_type": "text"
      },
      "source": [
        "Examine the distribution over labels on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3NbLPBhFOkp",
        "colab_type": "code",
        "outputId": "ffc27b23-a64a-4cbc-82e7-75a8a14816e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "discourse_counts = original_train_posts['discourse_type'].value_counts()\n",
        "print(discourse_counts.describe())\n",
        "\n",
        "top_discourse = discourse_counts.nlargest(20)\n",
        "print(top_discourse)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count       10.000000\n",
            "mean      7926.700000\n",
            "std       9664.321866\n",
            "min       1266.000000\n",
            "25%       1671.500000\n",
            "50%       3235.500000\n",
            "75%      11919.750000\n",
            "max      31419.000000\n",
            "Name: discourse_type, dtype: float64\n",
            "answer              31419\n",
            "elaboration         14775\n",
            "question            13610\n",
            "appreciation         6849\n",
            "agreement            3868\n",
            "disagreement         2603\n",
            "humor                1787\n",
            "other                1633\n",
            "announcement         1457\n",
            "negativereaction     1266\n",
            "Name: discourse_type, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTGOa5FQZtak",
        "colab_type": "text"
      },
      "source": [
        "# Q4 Text classification model for comment discourse prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlW933L_dTQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gp9hAKS6o59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "body_tf = TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf= True)\n",
        "body_tf.fit_transform(train_posts.body)\n",
        "author_tf = TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf= True) \n",
        "author_tf.fit_transform(train_posts.author)\n",
        "title_tf = TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf= True)\n",
        "title_tf.fit_transform(train_posts.title)\n",
        "prediction_pipeline = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('Tf_idf',body_tf), \n",
        "              ])),\n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('Tf_idf', author_tf), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('Tf_idf', title_tf), \n",
        "              ]))\n",
        "            ])\n",
        "        )\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bhb43l_6o90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "train1 = prediction_pipeline.fit_transform(train_posts)\n",
        "test1 = prediction_pipeline.transform(test_posts)\n",
        "validation1 = prediction_pipeline.transform(validation_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08c6dahf5G_j",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression with L2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10aYSSzEOvsP",
        "colab_type": "code",
        "outputId": "704824ea-b4c3-4e45-b25c-fc76fc188061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "lr4 = LogisticRegression(penalty='l2', C=10)\n",
        "lr4.fit(train1, train_labels)\n",
        "summary11=eval_summary(lr4.predict(test1), test_labels) "
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.475 P=0.262 R=0.334 F1=0.279\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.239     0.391     0.297       580\n",
            "    announcement      0.104     0.198     0.136       192\n",
            "          answer      0.693     0.529     0.600     10414\n",
            "    appreciation      0.588     0.672     0.627      1505\n",
            "    disagreement      0.045     0.168     0.071       173\n",
            "     elaboration      0.236     0.287     0.259      2994\n",
            "           humor      0.037     0.181     0.062        94\n",
            "negativereaction      0.101     0.263     0.146       118\n",
            "           other      0.085     0.184     0.116       174\n",
            "        question      0.486     0.467     0.476      3568\n",
            "\n",
            "        accuracy                          0.475     19812\n",
            "       macro avg      0.262     0.334     0.279     19812\n",
            "    weighted avg      0.542     0.475     0.501     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 227    4  402   52   13  147    3    9    9   85]\n",
            " [   1   38  176   13    2   69    2    1    6   57]\n",
            " [ 136   52 5504  158   61 1112   26   16   31  844]\n",
            " [  43   13  287 1011    3  142   10    7   39  165]\n",
            " [  18    4  365    7   29  122    0   10    3   83]\n",
            " [  93   32 1959  108   35  858   14   16   30  485]\n",
            " [   9    5  235   30    3   89   17    8    5   56]\n",
            " [   5    4  110   19    4   60    8   31    5   61]\n",
            " [  11   10  151   31    2   61    4    8   32   66]\n",
            " [  37   30 1225   76   21  334   10   12   14 1666]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn-TwjNC-7Bs",
        "colab_type": "text"
      },
      "source": [
        "# Error Analysis with validation posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akcoomGG_BW9",
        "colab_type": "code",
        "outputId": "d1ae4b98-3803-4ad4-eb26-a27e6b0c5df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "summary12=eval_summary(lr4.predict(validation1), validation_labels) "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.479 P=0.269 R=0.352 F1=0.287\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.269     0.419     0.328       503\n",
            "    announcement      0.122     0.158     0.138       215\n",
            "          answer      0.712     0.527     0.605      8454\n",
            "    appreciation      0.578     0.688     0.628      1138\n",
            "    disagreement      0.048     0.227     0.079       119\n",
            "     elaboration      0.255     0.310     0.280      2438\n",
            "           humor      0.051     0.274     0.086        73\n",
            "negativereaction      0.092     0.213     0.128       108\n",
            "           other      0.091     0.231     0.131       130\n",
            "        question      0.471     0.472     0.472      2676\n",
            "\n",
            "        accuracy                          0.479     15854\n",
            "       macro avg      0.269     0.352     0.287     15854\n",
            "    weighted avg      0.552     0.479     0.506     15854\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 211    5  330   30    6  107    0    8   16   70]\n",
            " [   5   34  115    9    2   58    3    2    7   43]\n",
            " [ 117   50 4452  127   30  824   14   16   20  603]\n",
            " [  43   26  232  783    2  110    7   15   13  124]\n",
            " [  12    5  319    9   27  110    2    3    2   75]\n",
            " [  70   42 1573   84   30  756   12   16   18  367]\n",
            " [   7   11  195   13    2   68   20   10    7   61]\n",
            " [   6    8  120   11    3   39    3   23    3   35]\n",
            " [   6    9  125   24    3   86    9    1   30   35]\n",
            " [  26   25  993   48   14  280    3   14   14 1263]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX906xoX_DnI",
        "colab_type": "text"
      },
      "source": [
        "# Dummy classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu5gwA_w6PnS",
        "colab_type": "code",
        "outputId": "f6aa4354-c5a8-4c51-e834-c017eb354ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "stratified_tf = DummyClassifier(strategy=\"stratified\")\n",
        "stratified_tf.fit(train1, train_labels)\n",
        "DS3 = ['Dummy Stratified', 'Tf-idf Vectorizer']\n",
        "prediction = stratified_tf.predict(test1)\n",
        "summary32 = eval_summary(prediction, test_labels, avg='macro')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.240 P=0.102 R=0.102 F1=0.102\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.049     0.048     0.049       971\n",
            "    announcement      0.014     0.014     0.014       349\n",
            "          answer      0.400     0.404     0.402      7857\n",
            "    appreciation      0.097     0.095     0.096      1756\n",
            "    disagreement      0.031     0.033     0.032       603\n",
            "     elaboration      0.187     0.183     0.185      3702\n",
            "           humor      0.018     0.019     0.018       427\n",
            "negativereaction      0.020     0.019     0.019       317\n",
            "           other      0.021     0.020     0.020       407\n",
            "        question      0.187     0.187     0.187      3423\n",
            "\n",
            "        accuracy                          0.240     19812\n",
            "       macro avg      0.102     0.102     0.102     19812\n",
            "    weighted avg      0.239     0.240     0.239     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[  47   25  381   94   23  172   19   17   21  152]\n",
            " [  34    5  147   31    5   72    4   10    6   51]\n",
            " [ 378  145 3175  726  227 1502  175  124  151 1337]\n",
            " [  90   26  673  166   62  302   38   27   38  298]\n",
            " [  34    9  242   64   20  145    9   12   12   94]\n",
            " [ 162   68 1445  275  132  677   82   49   88  652]\n",
            " [  24    5  172   50   17   85    8    7    7   82]\n",
            " [  17   11  128   20    7   45    5    6    9   59]\n",
            " [  16    4  152   40   10   71    7    9    8   59]\n",
            " [ 169   51 1342  290  100  631   80   56   67  639]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vreQHLNQUNFJ",
        "colab_type": "text"
      },
      "source": [
        "# Q5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7rEEXfgTEAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_posts_rich(file):\n",
        "  # A temporary variable to store the list of post content.\n",
        "  posts_tmp = list()\n",
        "  with open(file) as jsonfile:\n",
        "    for i, line in enumerate(jsonfile):\n",
        "      thread = json.loads(line)\n",
        "      thread_author = None\n",
        "      for post in thread['posts']:\n",
        "        # NOTE: This could be changed to use additional features from the post or thread.\n",
        "        # DO NOT change the labels for the test set.\n",
        "        discourse_type = post.get('majority_type', '')\n",
        "        post_depth = 0\n",
        "        if 'is_first_post' in post and post['is_first_post']:\n",
        "          thread_author = post.get('author', None)\n",
        "        else:\n",
        "          post_depth = post['post_depth']\n",
        "        features = [\n",
        "            thread[\"is_self_post\"],          \n",
        "            len(thread['posts']),            \n",
        "            post_depth,                      \n",
        "            post.get('author', None),        \n",
        "            thread_author,                   \n",
        "            thread['subreddit'],             \n",
        "            thread['title'],                 \n",
        "            post.get('body',''),            \n",
        "            discourse_type,                  \n",
        "        ]\n",
        "        posts_tmp.append(features)\n",
        "  # Create the posts data frame.  \n",
        "  labels = ['is_self_post','thread_length','post_depth','post_author','thread_author','subreddit','thread_title','body','discourse_type']\n",
        "  return pd.DataFrame(posts_tmp, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYAVU8sN5ncO",
        "colab_type": "text"
      },
      "source": [
        "# Load Train and Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC5BpP0td1ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_posts = load_posts_rich(discourse_train)\n",
        "train_posts = train_posts[train_posts['discourse_type'] != \"\"]\n",
        "test_posts = load_posts_rich(discourse_test)\n",
        "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
        "train_labels = train_posts['discourse_type']\n",
        "test_labels = test_posts['discourse_type']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjxqOYuq56gq",
        "colab_type": "text"
      },
      "source": [
        "# nltk library for tagged sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "morQn7KdTmOE",
        "colab_type": "code",
        "outputId": "06381308-5ec8-4a0f-ef99-90ae2e9ffd16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "nltk.download('treebank')\n",
        "corpus = nltk.corpus.treebank\n",
        "treebank_tagged_sentences = corpus.tagged_sents()\n",
        "tagged_sentences = [sentence for sentence in treebank_tagged_sentences]\n",
        "tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
        "tagger.train(tagged_sentences)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlGdwWRM6GKS",
        "colab_type": "text"
      },
      "source": [
        "# Function Transformer function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5xXU0Rlx3-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "def get_pos(string):\n",
        "  tags = [tag for word, tag in tagger.tag([a for a in string.split(' ') if a != ''])]\n",
        "  transformed = \" \".join(tags)\n",
        "  return transformed\n",
        "train_posts['pos'] = train_posts['body'].apply(get_pos)\n",
        "test_posts['pos'] = test_posts['body'].apply(get_pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WRK2MAcx6jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#content_punctuation\n",
        "post_vect = TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf= True)\n",
        "def get_text(df):\n",
        "  return df['body'].values \n",
        "select_text = FunctionTransformer(get_text, validate=False)\n",
        "content_punctuation_pipeline = Pipeline([\n",
        "      ('select_text', select_text), \n",
        "      ('vectorizer', post_vect)\n",
        "])\n",
        "normal_vectorizer =  TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf= True)\n",
        "normal_pipeline = Pipeline([\n",
        "      ('select_text', select_text), \n",
        "      ('vectorizer_normal', normal_vectorizer)\n",
        "])\n",
        "# Author\n",
        "import numpy as np\n",
        "def same_author(df):\n",
        "  tmp = []\n",
        "  for i, j in df.iterrows():\n",
        "    tmp.append(1 if j['post_author'] == j['thread_author'] else 0)\n",
        "  return np.array(tmp).reshape(len(tmp), 1)\n",
        "s_author = FunctionTransformer(same_author, validate=False)\n",
        "# Thread features\n",
        "def thread_length(df):\n",
        "  return df['thread_length'].values.reshape(len(df),1)\n",
        "t_length = FunctionTransformer(thread_length, validate=False)\n",
        "# Community:\n",
        "def get_subreddit(df):    \n",
        "  return df['subreddit'].values \n",
        "subreddit_hash_function = FunctionTransformer(get_subreddit, validate=False)\n",
        "subreddit_hash = Pipeline([\n",
        "      ('get_sub', subreddit_hash_function), \n",
        "      ('vectorizer', CountVectorizer())\n",
        "])\n",
        "# Word2Vec\n",
        "td = RegexpTokenizer(r'\\w+')\n",
        "def simple_tokenizer(string):\n",
        "  return [token.lower() for token in td.tokenize(string)]\n",
        "w2v_vectorizer = TfidfVectorizer(tokenizer=simple_tokenizer, sublinear_tf= True)\n",
        "def get_pos(df):\n",
        "  return df['pos'].values\n",
        "select_pos = FunctionTransformer(get_pos, validate=False)\n",
        "word_2_vec = Pipeline([\n",
        "      ('select_pos', select_pos), \n",
        "      ('w2v_vectorizer', w2v_vectorizer)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z1hqbUWO_Px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "prediction_pipeline= Pipeline([\n",
        "            ('union',FeatureUnion([\n",
        "              ('Content + Punctuation', content_punctuation_pipeline),\n",
        "              ('Author', s_author),\n",
        "              ('Thread features', t_length),\n",
        "              ('Community', subreddit_hash),\n",
        "              ('Word2vec', word_2_vec)\n",
        "            ]))\n",
        "])    \n",
        "train2 = prediction_pipeline.fit_transform(train_posts)\n",
        "test2 = prediction_pipeline.transform(test_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xCyF-kK6z7R",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00PJ9VDM65Ar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "1706c5ee-ffe1-4c37-9718-a816f2178b9b"
      },
      "source": [
        "lr5 = LogisticRegression(penalty='l2', C=10,solver='sag')\n",
        "lr5.fit(train2, train_labels)\n",
        "summary15=eval_summary(lr5.predict(test2),test_posts['discourse_type']) \n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classifier  has Acc=0.543 P=0.292 R=0.437 F1=0.308\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.184     0.501     0.269       349\n",
            "    announcement      0.395     0.465     0.427       310\n",
            "          answer      0.821     0.562     0.668     11595\n",
            "    appreciation      0.584     0.740     0.653      1356\n",
            "    disagreement      0.027     0.198     0.047        86\n",
            "     elaboration      0.223     0.334     0.268      2426\n",
            "           humor      0.028     0.295     0.052        44\n",
            "negativereaction      0.020     0.400     0.037        15\n",
            "           other      0.040     0.300     0.070        50\n",
            "        question      0.599     0.573     0.585      3581\n",
            "\n",
            "        accuracy                          0.543     19812\n",
            "       macro avg      0.292     0.437     0.308     19812\n",
            "    weighted avg      0.666     0.543     0.586     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 175    3  453   52    3  174    0    1    3   87]\n",
            " [   2  144   45    9    0   33    0    0    0  132]\n",
            " [  67   18 6522   75   32  719    6    0    7  494]\n",
            " [  22   18  375 1004    5  138    3    3    7  145]\n",
            " [   6    1  430    8   17  124    0    0    0   55]\n",
            " [  46   19 2203   70   13  810    4    2    8  455]\n",
            " [   2    9  282   27    1   59   13    2    5   57]\n",
            " [   6    0  173   20    1   55    1    6    3   42]\n",
            " [   3   14  177   46    1   50    7    0   15   63]\n",
            " [  20   84  935   45   13  264   10    1    2 2051]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}